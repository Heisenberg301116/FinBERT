import json
import os
from dotenv import load_dotenv 
import openai
import time
from termcolor import colored
import tiktoken


load_dotenv()
openai.api_key = os.getenv('api_key')

total_requests_till_now = 0
last_request_time = 0


def Limit_Max_Requests_per_minute():
  global total_requests_till_now
  global last_request_time
  
  if(total_requests_till_now == 0 or time.time() - last_request_time > 60 + 1):      # 60s = 1 min (+1 for safe side)
    last_request_time = time.time()
    total_requests_till_now = 1

  elif(time.time() - last_request_time <= 60 and total_requests_till_now < 3):   # max no. of requests = 3 per min 
    total_requests_till_now += 1

  else:                                                                  # wait for 1 min to be over for next API request.
    time_to_wait = (60+1) - (time.time() - last_request_time)      
    # +1s was done to compromise in some cases where python rounds off 45.2743492493243289 to 45.
    
    print(colored("Please wait for {} seconds.....".format(time_to_wait), 'red'))
    time.sleep(time_to_wait)
    last_request_time = time.time()
    total_requests_till_now = 1
        
  
  
def Call_OpenAI_Service(query, chunk, display_query_framed = False):
    instruction = "I will be giving you a question. After that, I will give you a context. You have to extract an answer from the context. The output should be the answer generated by you. If the answer is not present in the given context, return blank output."
  
    message = """
    %Question
    {}

    %Context
    {}

    %Your Answer
    """.format(query, chunk)

    prompt = [
        {"role": "system", "content": instruction},
        {"role": "user", "content": message},
    ]

    if(display_query_framed == True):
        print(colored("Query framed was: {}".format(prompt), 'blue'))

    Limit_Max_Requests_per_minute()

    response = openai.ChatCompletion.create(
        model='gpt-3.5-turbo',
        messages=prompt,
        temperature=0.5
    )   

    response_message = response["choices"][0]["message"]["content"]
    return response_message



def Total_tokens(message):
    encoding = tiktoken.encoding_for_model('gpt-3.5-turbo')
    return len(encoding.encode(message))